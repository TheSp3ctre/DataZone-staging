# Pipeline ETL BigQuery ‚Üí PostGIS

Documenta√ß√£o completa da pipeline de extra√ß√£o de dados da Anatel (BigQuery) para o PostGIS.

---

## üìã Vis√£o Geral

Esta pipeline ETL extrai dados de cobertura de fibra √≥ptica (FTTH/FTTB) do projeto **Base dos Dados** no BigQuery e carrega no banco PostGIS local, otimizada para evitar estouro de mem√≥ria.

**Fonte de Dados**: `basedosdados.br_anatel_banda_larga_fixa.microdados`

**Otimiza√ß√µes**:
- ‚úÖ Agrega√ß√£o server-side no BigQuery (reduz ~95% do tr√°fego)
- ‚úÖ Processamento em chunks (lazy loading)
- ‚úÖ Inser√ß√£o batch otimizada no PostgreSQL
- ‚úÖ Retry autom√°tico em caso de falhas de rede
- ‚úÖ Logs estruturados com m√©tricas de performance

---

## üîê Setup Inicial: Autentica√ß√£o Google Cloud

### 1. Criar Service Account no GCP

```bash
# 1. Acesse o Google Cloud Console
https://console.cloud.google.com/

# 2. Navegue para: IAM & Admin > Service Accounts

# 3. Clique em "Create Service Account"
#    - Nome: datazone-bigquery-reader
#    - Descri√ß√£o: Service Account para leitura BigQuery (DataZone Energy)

# 4. Conceder permiss√µes:
#    - BigQuery Data Viewer
#    - BigQuery Job User

# 5. Criar chave JSON:
#    - Clique na Service Account criada
#    - Aba "Keys" > "Add Key" > "Create new key"
#    - Tipo: JSON
#    - Baixar arquivo
```

### 2. Configurar Chave Localmente

```bash
# Copiar chave JSON para o diret√≥rio secrets/
cp ~/Downloads/datazone-bigquery-*.json secrets/gcp-service-account.json

# Verificar permiss√µes (read-only recomendado)
chmod 400 secrets/gcp-service-account.json
```

### 3. Configurar Vari√°veis de Ambiente

```bash
# Copiar template
cp .env.example .env

# Editar .env e verificar:
GCP_PROJECT_ID=basedosdados
GOOGLE_APPLICATION_CREDENTIALS=/secrets/gcp-service-account.json
```

---

## üöÄ Execu√ß√£o da Pipeline

### M√©todo 1: Docker Compose (Recomendado)

```bash
# 1. Build do container (primeira vez ou ap√≥s mudan√ßas)
docker-compose build pipeline_etl

# 2. Executar pipeline
docker-compose --profile etl run --rm pipeline_etl

# 3. Verificar logs em tempo real
tail -f logs/etl_anatel_*.log
```

### M√©todo 2: Execu√ß√£o Direta (Desenvolvimento)

```bash
# Dentro do container
docker-compose --profile etl run --rm pipeline_etl bash

# Executar script manualmente
python scripts/extrair_anatel.py
```

### M√©todo 3: Agendamento (Produ√ß√£o)

```bash
# Adicionar ao crontab para execu√ß√£o di√°ria √†s 2h da manh√£
0 2 * * * cd /path/to/datazone && docker-compose --profile etl run --rm pipeline_etl >> logs/cron.log 2>&1
```

---

## üìä Monitoramento

### Logs Estruturados

A pipeline gera logs em dois formatos:

1. **Console (stdout)**: Colorido, para acompanhamento em tempo real
2. **Arquivo**: JSON estruturado em `logs/etl_anatel_YYYY-MM-DD.log`

**Exemplo de log**:
```
2026-01-20 16:00:00 | INFO     | Pipeline ETL inicializada | Projeto: basedosdados | Chunk Size: 10000
2026-01-20 16:00:05 | SUCCESS  | ‚úÖ Conectado ao BigQuery | Projeto: basedosdados
2026-01-20 16:00:06 | SUCCESS  | ‚úÖ Conectado ao PostGIS | Vers√£o: 3.3
2026-01-20 16:00:10 | SUCCESS  | ‚úÖ Query executada com sucesso | Linhas: 5,432 | Tempo: 3.45s | Bytes processados: 12,345,678
2026-01-20 16:00:25 | SUCCESS  | ‚úÖ Inser√ß√£o conclu√≠da | Total de linhas: 5,432 | Tempo: 15.23s | Taxa: 356 linhas/s
2026-01-20 16:00:26 | SUCCESS  | ‚úÖ PIPELINE CONCLU√çDA COM SUCESSO | Tempo total: 26.12s
```

### M√©tricas Importantes

- **Linhas processadas**: Total de registros extra√≠dos do BigQuery
- **Bytes processados**: Volume de dados trafegados (BigQuery)
- **Tempo de execu√ß√£o**: Dura√ß√£o total da pipeline
- **Taxa de inser√ß√£o**: Linhas/segundo no PostgreSQL

### Verificar Dados no PostGIS

```sql
-- Conectar ao banco
docker-compose exec postgis psql -U datazone_user -d datazone_energy

-- Verificar total de registros
SELECT COUNT(*) FROM geo.cobertura_fibra;

-- Estat√≠sticas por UF
SELECT 
    uf,
    COUNT(*) as total_registros,
    SUM(total_acessos) as total_acessos,
    AVG(velocidade_media_mbps) as velocidade_media
FROM geo.cobertura_fibra
GROUP BY uf
ORDER BY total_acessos DESC;

-- Top 10 munic√≠pios com maior cobertura
SELECT 
    municipio,
    uf,
    tecnologia,
    total_acessos,
    total_operadoras,
    velocidade_media_mbps
FROM geo.cobertura_fibra
ORDER BY total_acessos DESC
LIMIT 10;
```

---

## üîß Troubleshooting

### Erro: "GOOGLE_APPLICATION_CREDENTIALS n√£o configurada"

**Causa**: Vari√°vel de ambiente n√£o definida ou chave JSON n√£o encontrada.

**Solu√ß√£o**:
```bash
# Verificar se arquivo existe
ls -la secrets/gcp-service-account.json

# Verificar vari√°vel no .env
grep GOOGLE_APPLICATION_CREDENTIALS .env

# Recriar container
docker-compose build pipeline_etl
```

### Erro: "Permission denied" ao acessar BigQuery

**Causa**: Service Account sem permiss√µes adequadas.

**Solu√ß√£o**:
1. Acesse GCP Console > IAM & Admin
2. Localize a Service Account
3. Adicione roles:
   - `BigQuery Data Viewer`
   - `BigQuery Job User`

### Erro: "Connection refused" ao conectar PostgreSQL

**Causa**: Container PostGIS n√£o est√° rodando ou n√£o passou no healthcheck.

**Solu√ß√£o**:
```bash
# Verificar status dos containers
docker-compose ps

# Verificar logs do PostGIS
docker-compose logs postgis

# Reiniciar PostGIS
docker-compose restart postgis

# Aguardar healthcheck (10-30 segundos)
docker-compose ps | grep postgis
```

### Erro: "Out of memory" durante processamento

**Causa**: Chunk size muito grande para mem√≥ria dispon√≠vel.

**Solu√ß√£o**:
```bash
# Reduzir chunk size no .env
ETL_CHUNK_SIZE=5000  # Padr√£o: 10000

# Ou passar diretamente
docker-compose --profile etl run --rm -e ETL_CHUNK_SIZE=5000 pipeline_etl
```

### Query BigQuery muito lenta

**Causa**: Tabela `basedosdados` pode estar com alta carga.

**Solu√ß√£o**:
- Executar em hor√°rios de menor uso (madrugada)
- Verificar status do BigQuery: https://status.cloud.google.com/

---

## üõ†Ô∏è Manuten√ß√£o

### Atualizar Query BigQuery

1. Editar arquivo `queries/extrair_fibra.sql`
2. Testar query no BigQuery Console
3. Rebuild container:
   ```bash
   docker-compose build pipeline_etl
   ```

### Adicionar Novas Fontes de Dados

1. Criar nova query em `queries/extrair_<fonte>.sql`
2. Duplicar `scripts/extrair_anatel.py` como `scripts/extrair_<fonte>.py`
3. Ajustar mapeamento de colunas e tabela destino
4. Adicionar ao `docker-compose.yml` (opcional: novo servi√ßo)

### Backup dos Dados

```bash
# Exportar tabela para CSV
docker-compose exec postgis psql -U datazone_user -d datazone_energy -c "\COPY geo.cobertura_fibra TO '/backups/cobertura_fibra.csv' CSV HEADER"

# Dump completo do schema geo
docker-compose exec postgis pg_dump -U datazone_user -d datazone_energy -n geo > backups/geo_schema_$(date +%Y%m%d).sql
```

---

## üìà Performance Tuning

### Otimiza√ß√µes Aplicadas

1. **BigQuery**:
   - Agrega√ß√£o server-side (GROUP BY no SQL)
   - Filtros aplicados antes da transfer√™ncia
   - Cache de queries habilitado

2. **Rede**:
   - Processamento em chunks (evita timeout)
   - Compress√£o autom√°tica (gzip)

3. **PostgreSQL**:
   - Inser√ß√£o batch (`method='multi'`)
   - √çndices criados ap√≥s inser√ß√£o (mais r√°pido)
   - Transa√ß√µes otimizadas

4. **Mem√≥ria**:
   - Lazy loading (n√£o carrega tudo na RAM)
   - Garbage collection autom√°tico entre chunks

### Benchmarks Esperados

| M√©trica | Valor T√≠pico |
|---------|--------------|
| Tempo total | 20-60 segundos |
| Taxa de inser√ß√£o | 300-500 linhas/s |
| Uso de mem√≥ria | < 1GB |
| Bytes processados (BigQuery) | 10-50 MB |

---

## üîí Seguran√ßa

### Checklist de Seguran√ßa

- ‚úÖ Chave JSON nunca commitada no Git (`.gitignore`)
- ‚úÖ Volume montado como read-only (`:ro`)
- ‚úÖ Credenciais via vari√°veis de ambiente
- ‚úÖ Conex√£o PostGIS via rede interna Docker
- ‚úÖ Limites de recursos configurados (preven√ß√£o DoS)
- ‚úÖ Logs n√£o cont√™m dados sens√≠veis

### Rota√ß√£o de Credenciais

```bash
# 1. Criar nova Service Account no GCP
# 2. Baixar nova chave JSON
# 3. Substituir arquivo
mv secrets/gcp-service-account.json secrets/gcp-service-account.json.old
cp ~/Downloads/nova-chave.json secrets/gcp-service-account.json

# 4. Testar
docker-compose --profile etl run --rm pipeline_etl

# 5. Deletar chave antiga no GCP Console
# 6. Remover arquivo local
rm secrets/gcp-service-account.json.old
```

---

## üìö Refer√™ncias

- [Base dos Dados - Documenta√ß√£o](https://basedosdados.org/)
- [BigQuery Python Client](https://cloud.google.com/python/docs/reference/bigquery/latest)
- [PostGIS Documentation](https://postgis.net/documentation/)
- [Docker Compose Profiles](https://docs.docker.com/compose/profiles/)

---

## üÜò Suporte

Em caso de d√∫vidas ou problemas:

1. Verificar logs: `tail -f logs/etl_anatel_*.log`
2. Consultar esta documenta√ß√£o
3. Abrir issue no reposit√≥rio do projeto
